---
title: "Topic Models"
output: html_document
---
```{r}
library(here)
library(dplyr)
library(tidytext)
library(tidyr)
library(knitr)
library(ggplot2)
library(tm)
library(topicmodels)
```

To do this analysis in this file, you will need to copy your saved 
long data into this project. 

In this project we are going to take a first look at a method of 
analysis called *topic models*. 
Topic models are a kind of machine learning that analyzes 

# Document-Term Matrix

So far we have been focused on *words* as our units.  For example, in the 
sentiment analysis each row represented one word, and there was a column that
represented the specific document (story, chapter, book, etc).   

In this analysis we will change perspective: the document will represent the
observation, so each document will will be a row.

The columns will represent "terms."  In this case the terms will be individual
words.   So each word acts like a variable.  The values in the column will 
be the frequency of that term in the specific document.  

These usually end up very wide (because there are a lot of words) and
relatively short (because there are not that many documents compared
to the number of words).

```{r}
fake_data_dtm <- data.frame(
  "Document" = c("Book 1", "Book 2", "Book 3"),
  "Cat" = c(0, 4, 1),
  "Sky" = c(0, 0, 1),
  "House" = c(3, 1, 1)
)
fake_data_dtm


```
```{r}
fake_data_long <- data.frame(
  "Word" = c("Cat", "Cat", "Sky", "House", "House", "House"),
  "n" = c(4, 1, 1, 3, 1, 1),
  "Document" = c("Book 2", "Book 3", "Book 3", "Book 1", "Book 2", "Book 3")
)
fake_data_long
```

```{r}
cast_dtm(fake_data_long, Word, Document, n) -> fake_dtm

fake_dtm$v
fake_dtm
```
```{r}
cast_sparse(fake_data_long, Word, Document, n) -> fake_sparse
fake_sparse

```

There are a few different packages that do this analysis. We are going to 
using the `tm` packages.

```{r}
load(here("quakersaints_long.rdata"))


quakersaints_long |> group_by(title, word) |> summarize(n = n()) |>
cast_dtm( term = word, document=title, n) -> quakersaints_dtm
```

# Latent Dirichlet allocation (LDA)

LDA "treats each document as a mixture of topics, and each topic as a mixture of words" (chapter 6).

This means that each document can have multiple topics.  
Each of the topics is made up of a mixture of words/terms.

We need to provide the parameter _k_  which is the number of topics to be 
identified.   In this sense topic models depend on the user, 
so you must have 
some judgement.  Often what we do is to try various values of _k_ to find the
best solution. 

The algorithm finds the best solution for a given number of topics. 
It requires a random number to start; we can specify a specific value which
will let us reproduce the same exact anaysis if we want.



```{r}
qs_lda_2 <-
  LDA( quakersaints_dtm, k = 2, control = list(seed = 1234))
qs_lda_2 
```
## For your data try starting with several different seeds. Do the results change?


The result object `qs_lda` contains a lot of different information. 
We can see that the word assignments are there in there. But how to 
access them?

We can use the `tidy()` function to make them more accessible.

```{r}
qs_topics_2 <- tidy(qs_lda_2, matrix = "beta")
head(qs_topics_2)
```

Notice that each row is a topic-term. That is, there is a row for each
combination of topic and word.  Since we set k =2 there are two rows per
word. with a higher k we will see k rows per term.

```{r}

qs_lda_3 <-
  LDA( quakersaints_dtm, k = 3, control = list(seed = 1234))
qs_topics_3 <- tidy(qs_lda_3, matrix = "beta")
head(qs_topics_3)
```



The `beta` colum represents the probability that the word was generated
for that *topic*.  That means we can find the words with the highest
probability for the topic.  

```{r}
qs_top_terms_2 <- qs_topics_2 |>
  group_by(topic) |>
  slice_max(beta, n = 10) |>
  ungroup() %>%
  arrange(topic, -beta)

qs_top_terms_2 |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
qs_top_terms_3 <- qs_topics_3 |>
  group_by(topic) |>
  slice_max(beta, n = 10) |>
  ungroup() %>%
  arrange(topic, -beta)

qs_top_terms_3 |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

Notice that the same words can be in multiple topics (george, god, day).

The following approach uses the ratio of the probabilities to find 
the words that differentiate the topics.

```{r}
beta_wide_2 <- qs_topics_2 |>
  mutate(topic = paste0("topic", topic)) |>
  pivot_wider(names_from = topic, values_from = beta) |>
  filter(topic1 > .001 | topic2 > .001) |>
  mutate(log_ratio = log2(topic2 / topic1))

beta_wide_2 |> slice_max(order_by = log_ratio, n = 10)
beta_wide_2 |> slice_min(order_by = log_ratio, n = 10)

```

The above looks at the words associated with a topic.  We can also look
at the probability we would guess the right topic based on the word. This uses
gamma.  

```{r}
qs_documents_2 <- tidy(qs_lda_2, matrix = "gamma")
qs_documents_2
```


```{r}
qs_documents_3 <- tidy(qs_lda_3, matrix = "gamma")
qs_documents_3
```

```{r}
top_terms_3 <- qs_topics_3 |>
  group_by(topic) |>
  slice_max(beta, n = 5) |>
  ungroup() |>
  arrange(topic, -beta)

top_terms_3

top_terms_3 %>%
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```
We can see that some words differentiate the topics and others do not.


For your analysis try different numbers of topics (these can include large
values of k, don't be afraid to choose 10 or maybe even more. )m

Write up a paragraph explaining what you learned about your documents.


```{r}

qs_lda_10 <-
  LDA( quakersaints_dtm, k = 10, control = list(seed = 1234))
qs_topics_10 <- tidy(qs_lda_10, matrix = "beta")
head(qs_topics_10,15)
```

```{r}
qs_top_terms_10 <- qs_topics_10 |>
  group_by(topic) |>
  slice_max(beta, n = 10) |>
  ungroup() %>%
  arrange(topic, -beta)

qs_top_terms_10 |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
